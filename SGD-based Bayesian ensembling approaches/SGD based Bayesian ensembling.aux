\relax 
\providecommand\hyper@newdestlabel[2]{}
\@writefile{toc}{\boolfalse {citerequest}\boolfalse {citetracker}\boolfalse {pagetracker}\boolfalse {backtracker}\relax }
\@writefile{lof}{\boolfalse {citerequest}\boolfalse {citetracker}\boolfalse {pagetracker}\boolfalse {backtracker}\relax }
\@writefile{lot}{\boolfalse {citerequest}\boolfalse {citetracker}\boolfalse {pagetracker}\boolfalse {backtracker}\relax }
\bibstyle{biblatex}
\bibdata{SGD_based_Bayesian_ensembling-blx,refs}
\citation{biblatex-control}
\abx@aux@refcontext{nyt/global//global/global}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{xie2013horizontal}
\abx@aux@cite{xie2013horizontal}
\abx@aux@segm{0}{0}{xie2013horizontal}
\citation{xie2013horizontal}
\abx@aux@cite{xie2013horizontal}
\abx@aux@segm{0}{0}{xie2013horizontal}
\citation{huang2017snapshot}
\abx@aux@cite{huang2017snapshot}
\abx@aux@segm{0}{0}{huang2017snapshot}
\citation{huang2017snapshot}
\abx@aux@cite{huang2017snapshot}
\abx@aux@segm{0}{0}{huang2017snapshot}
\citation{smith2017cyclical}
\abx@aux@cite{smith2017cyclical}
\abx@aux@segm{0}{0}{smith2017cyclical}
\citation{smith2017cyclical}
\abx@aux@cite{smith2017cyclical}
\abx@aux@segm{0}{0}{smith2017cyclical}
\citation{smith2017cyclical}
\abx@aux@cite{smith2017cyclical}
\abx@aux@segm{0}{0}{smith2017cyclical}
\citation{smith2017cyclical}
\abx@aux@cite{smith2017cyclical}
\abx@aux@segm{0}{0}{smith2017cyclical}
\abx@aux@page{2}{1}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {1}\textcite {xie2013horizontal}: Horizontal and vertical ensemble with deep representation for classification}{1}{section.1}\protected@file@percent }
\abx@aux@page{4}{1}
\abx@aux@page{6}{1}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {2}\textcite {smith2017cyclical}: Cyclical learning rates for training neural networks}{1}{section.2}\protected@file@percent }
\citation{smith2017cyclical}
\abx@aux@cite{smith2017cyclical}
\abx@aux@segm{0}{0}{smith2017cyclical}
\citation{smith2017cyclical}
\abx@aux@cite{smith2017cyclical}
\abx@aux@segm{0}{0}{smith2017cyclical}
\citation{loshchilov2016sgdr}
\abx@aux@cite{loshchilov2016sgdr}
\abx@aux@segm{0}{0}{loshchilov2016sgdr}
\citation{loshchilov2016sgdr}
\abx@aux@cite{loshchilov2016sgdr}
\abx@aux@segm{0}{0}{loshchilov2016sgdr}
\citation{smith2017cyclical}
\abx@aux@cite{smith2017cyclical}
\abx@aux@segm{0}{0}{smith2017cyclical}
\citation{smith2017cyclical}
\abx@aux@cite{smith2017cyclical}
\abx@aux@segm{0}{0}{smith2017cyclical}
\citation{gotmare2018using}
\abx@aux@cite{gotmare2018using}
\abx@aux@segm{0}{0}{gotmare2018using}
\citation{gotmare2018using}
\abx@aux@cite{gotmare2018using}
\abx@aux@segm{0}{0}{gotmare2018using}
\citation{brownlee2019snapshot}
\abx@aux@cite{brownlee2019snapshot}
\abx@aux@segm{0}{0}{brownlee2019snapshot}
\citation{brownlee2019snapshot}
\abx@aux@cite{brownlee2019snapshot}
\abx@aux@segm{0}{0}{brownlee2019snapshot}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Figure 2 from \textcite {smith2017cyclical}: Triangular learning rate policy. The blue lines represent learning rate values changing between bounds. The input parameter stepsize is the number of iterations in half a cycle.\relax }}{2}{figure.caption.1}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{triang}{{1}{2}{Figure 2 from \textcite {smith2017cyclical}: Triangular learning rate policy. The blue lines represent learning rate values changing between bounds. The input parameter stepsize is the number of iterations in half a cycle.\relax }{figure.caption.1}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Figure 3 from \textcite {smith2017cyclical}: Classification accuracy as a function of increasing learning rate for 8 epochs.\relax }}{2}{figure.caption.2}\protected@file@percent }
\newlabel{selectbounds}{{2}{2}{Figure 3 from \textcite {smith2017cyclical}: Classification accuracy as a function of increasing learning rate for 8 epochs.\relax }{figure.caption.2}{}}
\citation{huang2017snapshot}
\abx@aux@cite{huang2017snapshot}
\abx@aux@segm{0}{0}{huang2017snapshot}
\citation{huang2017snapshot}
\abx@aux@cite{huang2017snapshot}
\abx@aux@segm{0}{0}{huang2017snapshot}
\citation{loshchilov2016sgdr}
\abx@aux@cite{loshchilov2016sgdr}
\abx@aux@segm{0}{0}{loshchilov2016sgdr}
\citation{loshchilov2016sgdr}
\abx@aux@cite{loshchilov2016sgdr}
\abx@aux@segm{0}{0}{loshchilov2016sgdr}
\citation{huang2017snapshot}
\abx@aux@cite{huang2017snapshot}
\abx@aux@segm{0}{0}{huang2017snapshot}
\citation{huang2017snapshot}
\abx@aux@cite{huang2017snapshot}
\abx@aux@segm{0}{0}{huang2017snapshot}
\citation{loshchilov2016sgdr}
\abx@aux@cite{loshchilov2016sgdr}
\abx@aux@segm{0}{0}{loshchilov2016sgdr}
\citation{loshchilov2016sgdr}
\abx@aux@cite{loshchilov2016sgdr}
\abx@aux@segm{0}{0}{loshchilov2016sgdr}
\citation{huang2017snapshot}
\abx@aux@cite{huang2017snapshot}
\abx@aux@segm{0}{0}{huang2017snapshot}
\citation{huang2017snapshot}
\abx@aux@cite{huang2017snapshot}
\abx@aux@segm{0}{0}{huang2017snapshot}
\abx@aux@page{12}{3}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {3}\textcite {loshchilov2016sgdr}: SGDR: Stochastic gradient descent with warm restarts}{3}{section.3}\protected@file@percent }
\abx@aux@page{14}{3}
\abx@aux@page{16}{3}
\abx@aux@page{18}{3}
\abx@aux@page{20}{3}
\abx@aux@page{22}{3}
\abx@aux@page{24}{3}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {4}\textcite {huang2017snapshot}: Snapshot ensembles: Train 1, get M for free}{3}{section.4}\protected@file@percent }
\abx@aux@page{26}{3}
\citation{fort2020deep}
\abx@aux@cite{fort2020deep}
\abx@aux@segm{0}{0}{fort2020deep}
\citation{fort2020deep}
\abx@aux@cite{fort2020deep}
\abx@aux@segm{0}{0}{fort2020deep}
\citation{huang2017snapshot}
\abx@aux@cite{huang2017snapshot}
\abx@aux@segm{0}{0}{huang2017snapshot}
\citation{huang2017snapshot}
\abx@aux@cite{huang2017snapshot}
\abx@aux@segm{0}{0}{huang2017snapshot}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Figure 2 from \textcite {huang2017snapshot}: Training loss of 100-layer DenseNet on CIFAR10 using standard learning rate (blue) and M = 6 cosine annealing cycles (red). The intermediate models, denoted by the dotted lines, form an ensemble at the end of training.\relax }}{4}{figure.caption.3}\protected@file@percent }
\newlabel{huangschedule}{{3}{4}{Figure 2 from \textcite {huang2017snapshot}: Training loss of 100-layer DenseNet on CIFAR10 using standard learning rate (blue) and M = 6 cosine annealing cycles (red). The intermediate models, denoted by the dotted lines, form an ensemble at the end of training.\relax }{figure.caption.3}{}}
\abx@aux@page{30}{4}
\citation{huang2017snapshot}
\abx@aux@cite{huang2017snapshot}
\abx@aux@segm{0}{0}{huang2017snapshot}
\citation{huang2017snapshot}
\abx@aux@cite{huang2017snapshot}
\abx@aux@segm{0}{0}{huang2017snapshot}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Figure 5 from \textcite {huang2017snapshot}: Interpolations in parameter space between the final model (sixth snapshot) and all intermediate snapshots. $\lambda $ = 0 represents an intermediate snapshot model, while $\lambda $ = 1 represents the final model. Left: A Snapshot Ensemble, with cosine annealing cycles ($a_0$ = 0.2 every B/M = 50 epochs). Right: A NoCycle Snapshot Ensemble, (two learning rate drops, snapshots every 50 epochs). \relax }}{5}{figure.caption.4}\protected@file@percent }
\newlabel{huangminima}{{4}{5}{Figure 5 from \textcite {huang2017snapshot}: Interpolations in parameter space between the final model (sixth snapshot) and all intermediate snapshots. $\lambda $ = 0 represents an intermediate snapshot model, while $\lambda $ = 1 represents the final model. Left: A Snapshot Ensemble, with cosine annealing cycles ($a_0$ = 0.2 every B/M = 50 epochs). Right: A NoCycle Snapshot Ensemble, (two learning rate drops, snapshots every 50 epochs). \relax }{figure.caption.4}{}}
\citation{garipov2018loss}
\abx@aux@cite{garipov2018loss}
\abx@aux@segm{0}{0}{garipov2018loss}
\citation{garipov2018loss}
\abx@aux@cite{garipov2018loss}
\abx@aux@segm{0}{0}{garipov2018loss}
\citation{gotmare2018using}
\abx@aux@cite{gotmare2018using}
\abx@aux@segm{0}{0}{gotmare2018using}
\citation{gotmare2018using}
\abx@aux@cite{gotmare2018using}
\abx@aux@segm{0}{0}{gotmare2018using}
\citation{garipov2018loss}
\abx@aux@cite{garipov2018loss}
\abx@aux@segm{0}{0}{garipov2018loss}
\citation{garipov2018loss}
\abx@aux@cite{garipov2018loss}
\abx@aux@segm{0}{0}{garipov2018loss}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Figure 6 from \textcite {huang2017snapshot}: Pairwise correlation of softmax outputs between any two snapshots for DenseNet-100. Left:A Snapshot Ensemble, with cosine annealing cycles (restart with $a_0$ = 0.2 every 50 epochs). Right: A NoCycle Snapshot Ensemble, (two learning rate drops, snapshots every 50 epochs). \relax }}{6}{figure.caption.5}\protected@file@percent }
\newlabel{huangdiversity}{{5}{6}{Figure 6 from \textcite {huang2017snapshot}: Pairwise correlation of softmax outputs between any two snapshots for DenseNet-100. Left:A Snapshot Ensemble, with cosine annealing cycles (restart with $a_0$ = 0.2 every 50 epochs). Right: A NoCycle Snapshot Ensemble, (two learning rate drops, snapshots every 50 epochs). \relax }{figure.caption.5}{}}
\abx@aux@page{36}{6}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {5}\textcite {garipov2018loss}: Loss surfaces, mode connectivity and fast ensembling of DNNs}{6}{section.5}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Mode connectivity}{6}{subsection.5.1}\protected@file@percent }
\citation{garipov2018loss}
\abx@aux@cite{garipov2018loss}
\abx@aux@segm{0}{0}{garipov2018loss}
\citation{garipov2018loss}
\abx@aux@cite{garipov2018loss}
\abx@aux@segm{0}{0}{garipov2018loss}
\abx@aux@page{38}{7}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Figure 1 from \textcite {garipov2018loss}: The $l2$-regularized cross-entropy train loss surface of a ResNet-164 on CIFAR-100, as a function of network weights in a two-dimensional subspace. In each panel, the horizontal axis is fixed and is attached to the optima of two independently trained networks. The vertical axis changes between panels as we change planes (defined in the main text). Left: Three optima for independently trained networks. Middle and Right: A quadratic Bezier curve, and a polygonal chain with one bend, connecting the lower two optima on the left panel along a path of near-constant loss. Notice that in each panel a direct linear path between each mode would incur high loss. \relax }}{7}{figure.caption.6}\protected@file@percent }
\newlabel{modeconnect}{{6}{7}{Figure 1 from \textcite {garipov2018loss}: The $l2$-regularized cross-entropy train loss surface of a ResNet-164 on CIFAR-100, as a function of network weights in a two-dimensional subspace. In each panel, the horizontal axis is fixed and is attached to the optima of two independently trained networks. The vertical axis changes between panels as we change planes (defined in the main text). Left: Three optima for independently trained networks. Middle and Right: A quadratic Bezier curve, and a polygonal chain with one bend, connecting the lower two optima on the left panel along a path of near-constant loss. Notice that in each panel a direct linear path between each mode would incur high loss. \relax }{figure.caption.6}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Loss along the obtained path}{7}{subsection.5.2}\protected@file@percent }
\citation{smith2017cyclical}
\abx@aux@cite{smith2017cyclical}
\abx@aux@segm{0}{0}{smith2017cyclical}
\citation{smith2017cyclical}
\abx@aux@cite{smith2017cyclical}
\abx@aux@segm{0}{0}{smith2017cyclical}
\citation{huang2017snapshot}
\abx@aux@cite{huang2017snapshot}
\abx@aux@segm{0}{0}{huang2017snapshot}
\citation{huang2017snapshot}
\abx@aux@cite{huang2017snapshot}
\abx@aux@segm{0}{0}{huang2017snapshot}
\citation{izmailov2019averaging}
\abx@aux@cite{izmailov2019averaging}
\abx@aux@segm{0}{0}{izmailov2019averaging}
\citation{izmailov2019averaging}
\abx@aux@cite{izmailov2019averaging}
\abx@aux@segm{0}{0}{izmailov2019averaging}
\citation{garipov2018loss}
\abx@aux@cite{garipov2018loss}
\abx@aux@segm{0}{0}{garipov2018loss}
\citation{garipov2018loss}
\abx@aux@cite{garipov2018loss}
\abx@aux@segm{0}{0}{garipov2018loss}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Figure 2 from \textcite {garipov2018loss}: The $l2$-regularized cross-entropy train loss (left) and test error (middle) as a function of the point on the curves $\phi _{\theta }(t)$ found by the proposed method (ResNet-164 on CIFAR-100). Right: Error of the two-network ensemble consisting of the endpoint $\phi _{\theta }(0)$ of the curve and the point $\phi _{\theta }(t)$ on the curve (CIFAR-100, ResNet-164). ``Segment'' is a line segment connecting two modes found by SGD. “Polychain” is a polygonal chain connecting the same endpoints. \relax }}{8}{figure.caption.7}\protected@file@percent }
\newlabel{lossalong}{{7}{8}{Figure 2 from \textcite {garipov2018loss}: The $l2$-regularized cross-entropy train loss (left) and test error (middle) as a function of the point on the curves $\phi _{\theta }(t)$ found by the proposed method (ResNet-164 on CIFAR-100). Right: Error of the two-network ensemble consisting of the endpoint $\phi _{\theta }(0)$ of the curve and the point $\phi _{\theta }(t)$ on the curve (CIFAR-100, ResNet-164). ``Segment'' is a line segment connecting two modes found by SGD. “Polychain” is a polygonal chain connecting the same endpoints. \relax }{figure.caption.7}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Ensembling}{8}{subsection.5.3}\protected@file@percent }
\abx@aux@page{44}{8}
\abx@aux@page{46}{8}
\citation{mandt2017stochastic}
\abx@aux@cite{mandt2017stochastic}
\abx@aux@segm{0}{0}{mandt2017stochastic}
\citation{mandt2017stochastic}
\abx@aux@cite{mandt2017stochastic}
\abx@aux@segm{0}{0}{mandt2017stochastic}
\citation{izmailov2019averaging}
\abx@aux@cite{izmailov2019averaging}
\abx@aux@segm{0}{0}{izmailov2019averaging}
\citation{izmailov2019averaging}
\abx@aux@cite{izmailov2019averaging}
\abx@aux@segm{0}{0}{izmailov2019averaging}
\citation{izmailov2019averaging}
\abx@aux@cite{izmailov2019averaging}
\abx@aux@segm{0}{0}{izmailov2019averaging}
\citation{izmailov2019averaging}
\abx@aux@cite{izmailov2019averaging}
\abx@aux@segm{0}{0}{izmailov2019averaging}
\citation{izmailov2019averaging}
\abx@aux@cite{izmailov2019averaging}
\abx@aux@segm{0}{0}{izmailov2019averaging}
\citation{izmailov2019averaging}
\abx@aux@cite{izmailov2019averaging}
\abx@aux@segm{0}{0}{izmailov2019averaging}
\abx@aux@page{48}{9}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {6}\textcite {izmailov2019averaging}: Averaging weights leads to wider optima and better generalization}{9}{section.6}\protected@file@percent }
\abx@aux@page{50}{9}
\abx@aux@page{52}{9}
\abx@aux@page{54}{9}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Figure 1 from \textcite {izmailov2019averaging}: Illustrations of SWA and SGD with a Preactivation ResNet-164 on CIFAR-1001. Left: test error surface for three FGE samples and the corresponding SWA solution (averaging in weight space). Middle and Right: test error and train loss surfaces showing the weights proposed by SGD (at convergence) and SWA, starting from the same initialization of SGD after 125 training epochs.\relax }}{9}{figure.caption.8}\protected@file@percent }
\newlabel{averweights}{{8}{9}{Figure 1 from \textcite {izmailov2019averaging}: Illustrations of SWA and SGD with a Preactivation ResNet-164 on CIFAR-1001. Left: test error surface for three FGE samples and the corresponding SWA solution (averaging in weight space). Middle and Right: test error and train loss surfaces showing the weights proposed by SGD (at convergence) and SWA, starting from the same initialization of SGD after 125 training epochs.\relax }{figure.caption.8}{}}
\citation{izmailov2019averaging}
\abx@aux@cite{izmailov2019averaging}
\abx@aux@segm{0}{0}{izmailov2019averaging}
\citation{izmailov2019averaging}
\abx@aux@cite{izmailov2019averaging}
\abx@aux@segm{0}{0}{izmailov2019averaging}
\citation{izmailov2019averaging}
\abx@aux@cite{izmailov2019averaging}
\abx@aux@segm{0}{0}{izmailov2019averaging}
\citation{izmailov2019averaging}
\abx@aux@cite{izmailov2019averaging}
\abx@aux@segm{0}{0}{izmailov2019averaging}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Figure 3 from \textcite {izmailov2019averaging}: The l2-regularized cross-entropy train loss and test error surfaces of a Preactivation ResNet-164 on CIFAR100 in the plane containing the first, middle and last points (indicated by black crosses) in the trajectories with (left two) cyclical and (right two) constant learning rate schedules.\relax }}{10}{figure.caption.9}\protected@file@percent }
\newlabel{averweights2}{{9}{10}{Figure 3 from \textcite {izmailov2019averaging}: The l2-regularized cross-entropy train loss and test error surfaces of a Preactivation ResNet-164 on CIFAR100 in the plane containing the first, middle and last points (indicated by black crosses) in the trajectories with (left two) cyclical and (right two) constant learning rate schedules.\relax }{figure.caption.9}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Figure 4 from \textcite {izmailov2019averaging}: The l2-regularized cross-entropy train loss and test error surfaces of a Preactivation ResNet-164 on CIFAR100 in the plane containing the first, middle and last points (indicated by black crosses) in the trajectories with (left two) cyclical and (right two) constant learning rate schedules.\relax }}{10}{figure.caption.10}\protected@file@percent }
\newlabel{swawider}{{10}{10}{Figure 4 from \textcite {izmailov2019averaging}: The l2-regularized cross-entropy train loss and test error surfaces of a Preactivation ResNet-164 on CIFAR100 in the plane containing the first, middle and last points (indicated by black crosses) in the trajectories with (left two) cyclical and (right two) constant learning rate schedules.\relax }{figure.caption.10}{}}
\citation{polyak1992acceleration}
\abx@aux@cite{polyak1992acceleration}
\abx@aux@segm{0}{0}{polyak1992acceleration}
\citation{maddox2019simple}
\abx@aux@cite{maddox2019simple}
\abx@aux@segm{0}{0}{maddox2019simple}
\citation{maddox2019simple}
\abx@aux@cite{maddox2019simple}
\abx@aux@segm{0}{0}{maddox2019simple}
\citation{izmailov2019averaging}
\abx@aux@cite{izmailov2019averaging}
\abx@aux@segm{0}{0}{izmailov2019averaging}
\citation{izmailov2019averaging}
\abx@aux@cite{izmailov2019averaging}
\abx@aux@segm{0}{0}{izmailov2019averaging}
\citation{gur-ari2018gradient}
\abx@aux@cite{gur-ari2018gradient}
\abx@aux@segm{0}{0}{gur-ari2018gradient}
\citation{gur-ari2018gradient}
\abx@aux@cite{gur-ari2018gradient}
\abx@aux@segm{0}{0}{gur-ari2018gradient}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Figure 5 from \textcite {izmailov2019averaging}: l2-regularized cross-entropy train loss and test error as a function of a point on the line connecting SWA and SGD solutions on CIFAR-100. Left: Preactivation ResNet-164. Right: VGG-16.\relax }}{11}{figure.caption.11}\protected@file@percent }
\newlabel{swawider2}{{11}{11}{Figure 5 from \textcite {izmailov2019averaging}: l2-regularized cross-entropy train loss and test error as a function of a point on the line connecting SWA and SGD solutions on CIFAR-100. Left: Preactivation ResNet-164. Right: VGG-16.\relax }{figure.caption.11}{}}
\abx@aux@page{63}{11}
\abx@aux@page{65}{11}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {7}\textcite {maddox2019simple}: A simple baseline for Bayesian uncertainty in deep learning}{11}{section.7}\protected@file@percent }
\abx@aux@page{67}{11}
\abx@aux@page{69}{11}
\citation{maddox2019simple}
\abx@aux@cite{maddox2019simple}
\abx@aux@segm{0}{0}{maddox2019simple}
\citation{maddox2019simple}
\abx@aux@cite{maddox2019simple}
\abx@aux@segm{0}{0}{maddox2019simple}
\citation{gur-ari2018gradient}
\abx@aux@cite{gur-ari2018gradient}
\abx@aux@segm{0}{0}{gur-ari2018gradient}
\citation{gur-ari2018gradient}
\abx@aux@cite{gur-ari2018gradient}
\abx@aux@segm{0}{0}{gur-ari2018gradient}
\citation{maddox2019simple}
\abx@aux@cite{maddox2019simple}
\abx@aux@segm{0}{0}{maddox2019simple}
\citation{maddox2019simple}
\abx@aux@cite{maddox2019simple}
\abx@aux@segm{0}{0}{maddox2019simple}
\citation{maddox2019simple}
\abx@aux@cite{maddox2019simple}
\abx@aux@segm{0}{0}{maddox2019simple}
\citation{maddox2019simple}
\abx@aux@cite{maddox2019simple}
\abx@aux@segm{0}{0}{maddox2019simple}
\abx@aux@page{71}{12}
\abx@aux@page{73}{12}
\abx@aux@page{75}{12}
\citation{izmailov2019subspace}
\abx@aux@cite{izmailov2019subspace}
\abx@aux@segm{0}{0}{izmailov2019subspace}
\citation{izmailov2019subspace}
\abx@aux@cite{izmailov2019subspace}
\abx@aux@segm{0}{0}{izmailov2019subspace}
\citation{li2018measuring}
\abx@aux@cite{li2018measuring}
\abx@aux@segm{0}{0}{li2018measuring}
\citation{li2018measuring}
\abx@aux@cite{li2018measuring}
\abx@aux@segm{0}{0}{li2018measuring}
\citation{izmailov2019subspace}
\abx@aux@cite{izmailov2019subspace}
\abx@aux@segm{0}{0}{izmailov2019subspace}
\citation{izmailov2019subspace}
\abx@aux@cite{izmailov2019subspace}
\abx@aux@segm{0}{0}{izmailov2019subspace}
\citation{maddox2019simple}
\abx@aux@cite{maddox2019simple}
\abx@aux@segm{0}{0}{maddox2019simple}
\citation{maddox2019simple}
\abx@aux@cite{maddox2019simple}
\abx@aux@segm{0}{0}{maddox2019simple}
\citation{garipov2018loss}
\abx@aux@cite{garipov2018loss}
\abx@aux@segm{0}{0}{garipov2018loss}
\citation{garipov2018loss}
\abx@aux@cite{garipov2018loss}
\abx@aux@segm{0}{0}{garipov2018loss}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces Figure 1 from \textcite {maddox2019simple}: Left: Posterior joint density cross-sections along the rays corresponding to different eigenvectors of SWAG covariance matrix. Middle: Posterior joint density surface in the plane spanned by eigenvectors of SWAG covariance matrix corresponding to the first and second largest eigenvalues and (Right:) the third and fourth largest eigenvalues. All plots are produced using PreResNet-164 on CIFAR-100. The SWAG distribution projected onto these directions fits the geometry of the posterior density remarkably well (see discussion). \relax }}{13}{figure.caption.12}\protected@file@percent }
\newlabel{swaggeometry}{{12}{13}{Figure 1 from \textcite {maddox2019simple}: Left: Posterior joint density cross-sections along the rays corresponding to different eigenvectors of SWAG covariance matrix. Middle: Posterior joint density surface in the plane spanned by eigenvectors of SWAG covariance matrix corresponding to the first and second largest eigenvalues and (Right:) the third and fourth largest eigenvalues. All plots are produced using PreResNet-164 on CIFAR-100. The SWAG distribution projected onto these directions fits the geometry of the posterior density remarkably well (see discussion). \relax }{figure.caption.12}{}}
\abx@aux@page{79}{13}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {8}\textcite {izmailov2019subspace}: Subspace inference for Bayesian deep learning}{13}{section.8}\protected@file@percent }
\abx@aux@page{81}{13}
\abx@aux@page{83}{13}
\abx@aux@page{85}{13}
\abx@aux@page{87}{13}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{References}{15}{section.8}\protected@file@percent }
\abx@aux@page{88}{15}
\abx@aux@page{89}{15}
\abx@aux@page{90}{15}
\abx@aux@page{91}{15}
\abx@aux@page{92}{15}
\abx@aux@page{93}{15}
\abx@aux@page{94}{15}
\abx@aux@page{95}{15}
\abx@aux@page{96}{15}
\abx@aux@page{97}{15}
\abx@aux@page{98}{15}
\abx@aux@page{99}{15}
\abx@aux@page{100}{15}
\abx@aux@read@bbl@mdfivesum{07FC2D5E96A5F639EC7D98D18E058D90}
\abx@aux@refcontextdefaultsdone
\abx@aux@defaultrefcontext{0}{brownlee2019snapshot}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{fort2020deep}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{garipov2018loss}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{gotmare2018using}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{gur-ari2018gradient}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{huang2017snapshot}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{izmailov2019averaging}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{izmailov2019subspace}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{li2018measuring}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{loshchilov2016sgdr}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{maddox2019simple}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{mandt2017stochastic}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{polyak1992acceleration}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{smith2017cyclical}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{xie2013horizontal}{nyt/global//global/global}
\abx@aux@defaultlabelprefix{0}{brownlee2019snapshot}{}
\abx@aux@defaultlabelprefix{0}{fort2020deep}{}
\abx@aux@defaultlabelprefix{0}{garipov2018loss}{}
\abx@aux@defaultlabelprefix{0}{gotmare2018using}{}
\abx@aux@defaultlabelprefix{0}{gur-ari2018gradient}{}
\abx@aux@defaultlabelprefix{0}{huang2017snapshot}{}
\abx@aux@defaultlabelprefix{0}{izmailov2019averaging}{}
\abx@aux@defaultlabelprefix{0}{izmailov2019subspace}{}
\abx@aux@defaultlabelprefix{0}{li2018measuring}{}
\abx@aux@defaultlabelprefix{0}{loshchilov2016sgdr}{}
\abx@aux@defaultlabelprefix{0}{maddox2019simple}{}
\abx@aux@defaultlabelprefix{0}{mandt2017stochastic}{}
\abx@aux@defaultlabelprefix{0}{polyak1992acceleration}{}
\abx@aux@defaultlabelprefix{0}{smith2017cyclical}{}
\abx@aux@defaultlabelprefix{0}{xie2013horizontal}{}
\abx@aux@page{101}{16}
\abx@aux@page{102}{16}
\gdef \@abspage@last{16}
