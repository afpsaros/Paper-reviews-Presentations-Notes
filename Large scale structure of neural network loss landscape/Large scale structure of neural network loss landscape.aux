\relax 
\providecommand\hyper@newdestlabel[2]{}
\@writefile{toc}{\boolfalse {citerequest}\boolfalse {citetracker}\boolfalse {pagetracker}\boolfalse {backtracker}\relax }
\@writefile{lof}{\boolfalse {citerequest}\boolfalse {citetracker}\boolfalse {pagetracker}\boolfalse {backtracker}\relax }
\@writefile{lot}{\boolfalse {citerequest}\boolfalse {citetracker}\boolfalse {pagetracker}\boolfalse {backtracker}\relax }
\bibstyle{biblatex}
\bibdata{Large_scale_structure_of_neural_network_loss_landscape-blx,refs}
\citation{biblatex-control}
\abx@aux@refcontext{nyt/global//global/global}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{fort2020deep}
\abx@aux@cite{fort2020deep}
\abx@aux@segm{0}{0}{fort2020deep}
\citation{fort2020deep}
\abx@aux@cite{fort2020deep}
\abx@aux@segm{0}{0}{fort2020deep}
\citation{fort2019large}
\abx@aux@cite{fort2019large}
\abx@aux@segm{0}{0}{fort2019large}
\citation{fort2019large}
\abx@aux@cite{fort2019large}
\abx@aux@segm{0}{0}{fort2019large}
\citation{fort2020deep}
\abx@aux@cite{fort2020deep}
\abx@aux@segm{0}{0}{fort2020deep}
\citation{fort2020deep}
\abx@aux@cite{fort2020deep}
\abx@aux@segm{0}{0}{fort2020deep}
\citation{wilson2020bayesian}
\abx@aux@cite{wilson2020bayesian}
\abx@aux@segm{0}{0}{wilson2020bayesian}
\citation{wilson2020bayesian}
\abx@aux@cite{wilson2020bayesian}
\abx@aux@segm{0}{0}{wilson2020bayesian}
\abx@aux@page{2}{1}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {1}Summary}{1}{section.1}\protected@file@percent }
\newlabel{sec:introduction}{{1}{1}{Summary}{section.1}{}}
\abx@aux@page{4}{1}
\abx@aux@page{6}{1}
\abx@aux@page{8}{1}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {2}Two definitions}{1}{section.2}\protected@file@percent }
\citation{li2018visualizing}
\abx@aux@cite{li2018visualizing}
\abx@aux@segm{0}{0}{li2018visualizing}
\citation{li2018visualizing}
\abx@aux@cite{li2018visualizing}
\abx@aux@segm{0}{0}{li2018visualizing}
\citation{garipov2018loss}
\abx@aux@cite{garipov2018loss}
\abx@aux@segm{0}{0}{garipov2018loss}
\citation{garipov2018loss}
\abx@aux@cite{garipov2018loss}
\abx@aux@segm{0}{0}{garipov2018loss}
\citation{draxler2018essentially}
\abx@aux@cite{draxler2018essentially}
\abx@aux@segm{0}{0}{draxler2018essentially}
\citation{draxler2018essentially}
\abx@aux@cite{draxler2018essentially}
\abx@aux@segm{0}{0}{draxler2018essentially}
\citation{fort2019large}
\abx@aux@cite{fort2019large}
\abx@aux@segm{0}{0}{fort2019large}
\citation{fort2019large}
\abx@aux@cite{fort2019large}
\abx@aux@segm{0}{0}{fort2019large}
\citation{wilson2020bayesian}
\abx@aux@cite{wilson2020bayesian}
\abx@aux@segm{0}{0}{wilson2020bayesian}
\citation{wilson2020bayesian}
\abx@aux@cite{wilson2020bayesian}
\abx@aux@segm{0}{0}{wilson2020bayesian}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {3}Discussion of figures}{2}{section.3}\protected@file@percent }
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Figure 2 a and b from the paper: a) Cosine similarity of weights along an optimization trajectory. After checkpoint 10, weights within a trajectory are very similar. b) Disagreement of functions obtained along an optimization trajectory. After checkpoint 10, models predict very similar labels. Predictions are not very diverse.\relax }}{2}{figure.caption.1}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{}{{1}{2}{Figure 2 a and b from the paper: a) Cosine similarity of weights along an optimization trajectory. After checkpoint 10, weights within a trajectory are very similar. b) Disagreement of functions obtained along an optimization trajectory. After checkpoint 10, models predict very similar labels. Predictions are not very diverse.\relax }{figure.caption.1}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Figure 2 c from the paper: 2-D t-SNE projections of predictions obtained from weights along 3 different optimization trajectories. Projections corresponding to the same trajectory (color) are very close to each other but far from predictions from different trajectories. This shows that weights within a trajectory give rise to very similar predictive models but different trajectories give rise to very different models.\relax }}{2}{figure.caption.2}\protected@file@percent }
\newlabel{}{{2}{2}{Figure 2 c from the paper: 2-D t-SNE projections of predictions obtained from weights along 3 different optimization trajectories. Projections corresponding to the same trajectory (color) are very close to each other but far from predictions from different trajectories. This shows that weights within a trajectory give rise to very similar predictive models but different trajectories give rise to very different models.\relax }{figure.caption.2}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Figure 3 a from the paper: Left: Cosine similarity between optima (final weights in each trajectory) from different optimization trajectory. Different optima are almost orthogonal to each other. Right: Corresponding predictive models/functions obtained disagree on predictions.\relax }}{3}{figure.caption.3}\protected@file@percent }
\newlabel{}{{3}{3}{Figure 3 a from the paper: Left: Cosine similarity between optima (final weights in each trajectory) from different optimization trajectory. Different optima are almost orthogonal to each other. Right: Corresponding predictive models/functions obtained disagree on predictions.\relax }{figure.caption.3}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Figure 4 from the paper: 2-D t-SNE projections of predictions obtained from 3 different optimization trajectories. Additional predictions (less opacity in figure) are added by using one of the 4 sampling techniques (here dropout). Remember these 4 sampling techniques at zero or low cost sample more weights along a single optimization trajectory. It is seen that the sampling techniques only help better explore a single mode and the predictions are not very diverse.\relax }}{3}{figure.caption.4}\protected@file@percent }
\newlabel{}{{4}{3}{Figure 4 from the paper: 2-D t-SNE projections of predictions obtained from 3 different optimization trajectories. Additional predictions (less opacity in figure) are added by using one of the 4 sampling techniques (here dropout). Remember these 4 sampling techniques at zero or low cost sample more weights along a single optimization trajectory. It is seen that the sampling techniques only help better explore a single mode and the predictions are not very diverse.\relax }{figure.caption.4}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Figure 5 left from the paper: Having 3 points in the w-space we can create a unique 2-D plane passing through these 3 points. See for example \textcite {li2018visualizing} and \textit  {losslandscape.com} where they create a plane connecting one weight plus 2 random directions from it. In this figure the 3 points for creating a plane are the origin and the 2 optima (stars) found by 2 different optimization trajectories. The dots show the 2 trajectories as projected on this plane (as far as I understand it). It seems that because of high dimensionality the points along the optimization trajectories fall very close to the lines connecting the origin and the optima. It is also seen that the 2 optima achieve similar training loss while if we tried to connect the 2 optima with a straight line we would have to pass through a region of high loss. Connecting ``tunnels'' between 2 optima can be found with the techniques of \textcite {garipov2018loss} and \textcite {draxler2018essentially}. See also \textcite {fort2019large} for tunnels connecting M modes. These tunnels are useful because we can use the different weights in the tunnel for ensembling. In order to produce this plot we need to go at each pixel in the plot (corresponding to a weight vector) and compute its training loss. Finally, with pink you can see the different weights obtained by using one of the 4 sampling techniques. They are very close to the optimum of the 1st trajectory and they are orthogonal to the optimum of the 2nd trajectory. \relax }}{4}{figure.caption.5}\protected@file@percent }
\newlabel{}{{5}{4}{Figure 5 left from the paper: Having 3 points in the w-space we can create a unique 2-D plane passing through these 3 points. See for example \textcite {li2018visualizing} and \textit {losslandscape.com} where they create a plane connecting one weight plus 2 random directions from it. In this figure the 3 points for creating a plane are the origin and the 2 optima (stars) found by 2 different optimization trajectories. The dots show the 2 trajectories as projected on this plane (as far as I understand it). It seems that because of high dimensionality the points along the optimization trajectories fall very close to the lines connecting the origin and the optima. It is also seen that the 2 optima achieve similar training loss while if we tried to connect the 2 optima with a straight line we would have to pass through a region of high loss. Connecting ``tunnels'' between 2 optima can be found with the techniques of \textcite {garipov2018loss} and \textcite {draxler2018essentially}. See also \textcite {fort2019large} for tunnels connecting M modes. These tunnels are useful because we can use the different weights in the tunnel for ensembling. In order to produce this plot we need to go at each pixel in the plot (corresponding to a weight vector) and compute its training loss. Finally, with pink you can see the different weights obtained by using one of the 4 sampling techniques. They are very close to the optimum of the 1st trajectory and they are orthogonal to the optimum of the 2nd trajectory. \relax }{figure.caption.5}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Figure 5 middle from the paper: Function similarity of each model in the plane as compared to the 1st optimum. This shows that when we leave the low loss valley around the weight the predictions are very different. Finally, with pink you can see the function similarities of the weights obtained by using one of the 4 sampling techniques. The functions obtained are very similar to the function obtained using the optimum of the 1st trajectory.\relax }}{5}{figure.caption.6}\protected@file@percent }
\newlabel{}{{6}{5}{Figure 5 middle from the paper: Function similarity of each model in the plane as compared to the 1st optimum. This shows that when we leave the low loss valley around the weight the predictions are very different. Finally, with pink you can see the function similarities of the weights obtained by using one of the 4 sampling techniques. The functions obtained are very similar to the function obtained using the optimum of the 1st trajectory.\relax }{figure.caption.6}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Figure 6 left from the paper: Y-axis is function disagreement of each sample with baseline optimum. Green star shows the single optimum: zero diversity because it is compared with itself. Dots show diversity (function disagreement) vs accuracy for the samples obtained via the 4 sampling techniques. For example, you can get more diverse predictions using the 4 sampling techniques by increasing the noise. But this comes at the cost of accuracy because you leave the loss valley shown in figure 5 of the paper and you visit a high-loss region. With red the predictions by different trajectories are shown. We can see that there is no trade off for different trajectories. We have similar accuracy and very different predictions.\relax }}{6}{figure.caption.7}\protected@file@percent }
\newlabel{}{{7}{6}{Figure 6 left from the paper: Y-axis is function disagreement of each sample with baseline optimum. Green star shows the single optimum: zero diversity because it is compared with itself. Dots show diversity (function disagreement) vs accuracy for the samples obtained via the 4 sampling techniques. For example, you can get more diverse predictions using the 4 sampling techniques by increasing the noise. But this comes at the cost of accuracy because you leave the loss valley shown in figure 5 of the paper and you visit a high-loss region. With red the predictions by different trajectories are shown. We can see that there is no trade off for different trajectories. We have similar accuracy and very different predictions.\relax }{figure.caption.7}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Figure 8: Left: Here the authors use deep ensembles plus additional samples via the 4 sampling techniques. Remember each optimum in the ensemble has an optimization trajectory that we can use to sample more weights using one of the 4 sampling techniques. What they do here is similar to Multi-SWAG proposed in \textcite {wilson2020bayesian}. It shows that ensembling plus sampling can be a little better than ensembling. Ensembling is for sure better than just sampling however in terms of generalization. Right: Same for calibration (uncertainty quality). Small Brier score means better calibrated uncertainty.\relax }}{7}{figure.caption.8}\protected@file@percent }
\newlabel{}{{8}{7}{Figure 8: Left: Here the authors use deep ensembles plus additional samples via the 4 sampling techniques. Remember each optimum in the ensemble has an optimization trajectory that we can use to sample more weights using one of the 4 sampling techniques. What they do here is similar to Multi-SWAG proposed in \textcite {wilson2020bayesian}. It shows that ensembling plus sampling can be a little better than ensembling. Ensembling is for sure better than just sampling however in terms of generalization. Right: Same for calibration (uncertainty quality). Small Brier score means better calibrated uncertainty.\relax }{figure.caption.8}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{References}{7}{figure.caption.8}\protected@file@percent }
\abx@aux@page{19}{7}
\abx@aux@page{20}{7}
\abx@aux@page{21}{7}
\abx@aux@page{22}{7}
\abx@aux@page{23}{7}
\abx@aux@read@bbl@mdfivesum{0C8CC5AF0B625B536F0829D74B96ADEC}
\abx@aux@refcontextdefaultsdone
\abx@aux@defaultrefcontext{0}{draxler2018essentially}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{fort2020deep}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{fort2019large}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{garipov2018loss}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{li2018visualizing}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{wilson2020bayesian}{nyt/global//global/global}
\abx@aux@defaultlabelprefix{0}{draxler2018essentially}{}
\abx@aux@defaultlabelprefix{0}{fort2020deep}{}
\abx@aux@defaultlabelprefix{0}{fort2019large}{}
\abx@aux@defaultlabelprefix{0}{garipov2018loss}{}
\abx@aux@defaultlabelprefix{0}{li2018visualizing}{}
\abx@aux@defaultlabelprefix{0}{wilson2020bayesian}{}
\abx@aux@page{24}{8}
\gdef \@abspage@last{8}
